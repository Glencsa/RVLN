import os
import torch
import torch.nn as nn
from transformers import (
    InstructBlipProcessor,
    InstructBlipConfig,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)

# å¼•å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å—
from models.InstructBlip import InstructBlipMultiTask 
# å¼•å…¥ä½ ä¸Šé¢æä¾›çš„ Dataset å’Œ Collator ç±»
from data_utils import InstructBlipLoRADataset, DataCollatorForInstructBlip 

def print_trainable_parameters(model):
    """æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || "
        f"trainable%: {100 * trainable_params / all_param:.2f}"
    )

# ==========================================
# 1. ä¿®æ­£ Data Collator ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥
# ==========================================
class DataCollatorWrapper(DataCollatorForInstructBlip):
    """
    åŒ…è£…ä½ åŸæœ¬çš„ Collatorï¼Œå°†è¾“å‡ºçš„é”®åä¿®æ”¹ä¸ºæ¨¡å‹ forward å‡½æ•°éœ€è¦çš„åå­—
    pixel_values_rgb -> pixel_values
    pixel_values_depth -> depth_pixel_values
    """
    def __call__(self, batch):
        outputs = super().__call__(batch)
        
        # é‡å‘½åé”®å€¼ä»¥åŒ¹é… InstructBlipMultiTask.forward çš„å‚æ•°
        if "pixel_values_rgb" in outputs:
            outputs["pixel_values"] = outputs.pop("pixel_values_rgb")
        
        if "pixel_values_depth" in outputs:
            outputs["depth_pixel_values"] = outputs.pop("pixel_values_depth")
            
        return outputs

# ==========================================
# 2. è‡ªå®šä¹‰ Trainer (ç¡®ä¿ä¿å­˜ Embeddings)
# ==========================================
class CustomTrainer(Trainer):
    def save_model(self, output_dir=None, _internal_call=False):
        """é‡å†™ä¿å­˜é€»è¾‘ï¼Œç¡®ä¿ LoRA + Embeddings + Tokenizer éƒ½èƒ½è¢«ä¿å­˜"""
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜ LoRA å’Œ modules_to_save (embed_tokens)
        super().save_model(output_dir, _internal_call)
        
        # 2. ä¿å­˜ Tokenizer
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"âœ… Model (LoRA + Embeddings) saved to {output_dir}")

def main():
    # =================Configuration=================
    model_name_or_path = "./instructblip-vicuna-7b" 
    # ä¹‹å‰è®­ç»ƒå¥½çš„ Stage 1 æƒé‡è·¯å¾„ (åŒ…å« Fusion, Q-Former, Depth ç­‰)
    stage1_checkpoint = "checkpoints_itm_cross_attn_with_depth_qformer_vit_v1/latest_checkpoint.pth"
    
    data_path = "/home/isvl/guan_code/RVLN/datasets/filtered_traj_3279.json"
    output_dir = "./output/instructblip_sft_llm"
    
    # è®­ç»ƒå‚æ•°
    batch_size = 2 
    grad_accumulation = 8 # ç¨å¾®åŠ å¤§ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    learning_rate = 5e-5  # SFT LLM å­¦ä¹ ç‡
    num_epochs = 3
    
    # =================1. Processor & Tokenizer=================
    print("Loading Processor...")
    processor = InstructBlipProcessor.from_pretrained(model_name_or_path)
    tokenizer = processor.tokenizer
    qformer_tokenizer = processor.qformer_tokenizer

    # æ·»åŠ ç‰¹æ®Š Token
    special_tokens_dict = {'additional_special_tokens': ["<history>", "<current>"]}
    tokenizer.add_special_tokens(special_tokens_dict)
    
    history_token_id = tokenizer.convert_tokens_to_ids("<history>")
    current_token_id = tokenizer.convert_tokens_to_ids("<current>")

    # =================2. Model Initialization=================
    print("Loading Base Model...")
    config = InstructBlipConfig.from_pretrained(model_name_or_path)
    config.history_token_id = history_token_id
    config.current_token_id = current_token_id

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = InstructBlipMultiTask.from_pretrained(
        model_name_or_path,
        config=config,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    # è°ƒæ•´ Embedding å¤§å° (å¿…é¡»åœ¨åŠ è½½ Stage 1 æƒé‡å‰åšï¼Œå¦åˆ™ç»´åº¦å¯¹ä¸ä¸Š)
    model.language_model.resize_token_embeddings(len(tokenizer))

    # =================3. [å…³é”®] åŠ è½½ Stage 1 è®­ç»ƒå¥½çš„æƒé‡=================
    if os.path.exists(stage1_checkpoint):
        print(f"ğŸ“¥ Loading Stage 1 Checkpoint from: {stage1_checkpoint}")
        ckpt = torch.load(stage1_checkpoint, map_location="cpu")
        
        # åŠ è½½å„ä¸ªæ¨¡å—
        msg = model.load_state_dict(ckpt, strict=False) 
        # strict=False æ˜¯å¿…é¡»çš„ï¼Œå› ä¸º ckpt é‡Œå¯èƒ½æ²¡æœ‰ LLM çš„æƒé‡ï¼Œåªæœ‰ fusion/qformer ç­‰
        print(f"Checkpoint Load Status: {msg}")
        
        # éªŒè¯å…³é”®æ¨¡å—æ˜¯å¦åŠ è½½ (ç®€å•æ£€æŸ¥ key)
        if 'visual_fusion' in ckpt: print(" - Visual Fusion Loaded âœ…")
        if 'qformer' in ckpt: print(" - Q-Former Loaded âœ…")
        if 'depth_backbone' in ckpt: print(" - Depth Backbone Loaded âœ…")
        
        # âš ï¸ é‡è¦ï¼šå¦‚æœ Stage 1 è®­ç»ƒæ—¶ä¹Ÿ resize äº† embedding å¹¶ä¸”ä¿å­˜äº†ï¼Œ
        # è¿™é‡Œçš„ load_state_dict å¯èƒ½ä¼šè¦†ç›–æ‰åˆšåˆš resize çš„ embeddingã€‚
        # å¦‚æœ Stage 1 æ²¡ä¿å­˜ LLM embeddingï¼Œåˆ™è¿™é‡Œæ˜¯ä»å¤´è®­ç»ƒ embeddingã€‚
    else:
        print("âŒ Warning: Stage 1 checkpoint not found! Training from scratch (Not Recommended).")

    # =================4. Freeze & LoRA Setup=================
    
    # 4.1 å…¨å±€å†»ç»“
    for param in model.parameters():
        param.requires_grad = False
        
    # 4.2 é…ç½® LoRA (é’ˆå¯¹ LLM)
    peft_config = LoraConfig(
        r=32, # ç¨å¾®åŠ å¤§ rank ä»¥æå‡ LLM è¡¨ç°
        lora_alpha=64,
        # é’ˆå¯¹ Vicuna/Llama çš„æ‰€æœ‰çº¿æ€§å±‚
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        # âš ï¸ å…³é”®ï¼šå› ä¸ºåŠ äº†æ–° tokenï¼Œå¿…é¡»è®­ç»ƒ Embedding å±‚å’Œ Head
        modules_to_save=["embed_tokens", "lm_head"] 
    )
    
    print("Applying LoRA to LLM...")
    model.language_model = get_peft_model(model.language_model, peft_config)
    
    # 4.3 ç¡®è®¤å…¶ä»–éƒ¨åˆ†ä¿æŒå†»ç»“
    # åœ¨ SFT é˜¶æ®µï¼Œé€šå¸¸æˆ‘ä»¬å†»ç»“è§†è§‰éƒ¨åˆ†ï¼ˆFusion, QFormer, Depthï¼‰ï¼Œåªè°ƒ LLMã€‚
    # è¿™æ ·å¯ä»¥é˜²æ­¢ LLM çš„æ¢¯åº¦ç ´åå·²ç»å¯¹é½å¥½çš„è§†è§‰ç‰¹å¾ã€‚
    # å¦‚æœä½ æƒ³ç»§ç»­å¾®è°ƒ Fusionï¼Œå¯ä»¥åœ¨è¿™é‡Œè§£å†»å®ƒï¼Œä½†é€šå¸¸ä¸å»ºè®®åŒæ—¶åšã€‚
    
    print_trainable_parameters(model)

    # =================5. Data Setup=================
    print("Loading Dataset...")
    train_dataset = InstructBlipLoRADataset(
        data_path=data_path,
        processor=processor,
        tokenizer=tokenizer,
        image_root="", # å¡«å…¥ä½ çš„å›¾ç‰‡æ ¹ç›®å½•
        history_len=4,
        current_len=1
    )
    
    # ä½¿ç”¨ Wrapper åçš„ Collator
    collator = DataCollatorWrapper(
        processor=processor,
        tokenizer=tokenizer,
        qformer_tokenizer=qformer_tokenizer
    )

    # =================6. Trainer Setup=================
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accumulation,
        learning_rate=learning_rate,
        logging_steps=10,
        save_strategy="epoch",
        num_train_epochs=num_epochs,
        bf16=True,
        remove_unused_columns=False,
        report_to="tensorboard",
        save_total_limit=2,
    )

    # ä½¿ç”¨è‡ªå®šä¹‰ Trainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=collator,
        tokenizer=tokenizer
    )

    # =================7. Training=================
    print("Starting SFT Training...")
    trainer.train()
    
    trainer.save_model(output_dir)

if __name__ == "__main__":
    main()