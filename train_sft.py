import os
import torch
import torch.nn as nn
import torch.distributed as dist
import numpy as np 
import torch.nn.functional as F
from torch.utils.data import random_split
import swanlab
from transformers import (
    InstructBlipProcessor,
    InstructBlipConfig,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)
from swanlab.integration.huggingface import SwanLabCallback
from models.WayPointVLN import RvlnMultiTask 
from utils.data_utils import RvlnLoRADataset, DataCollatorForRvln
from utils.utils import *

class WeightedTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # ==================== 1. åˆå§‹åŒ– Token æ˜ å°„ä¸è¶…å‚æ•° ====================
        self.target_token_ids = set() # ç”¨äºåŸºç¡€åŠ æƒ Mask (åŒ…å« -1)
        self.id_to_value = {}         # ç”¨äºè·ç¦»è®¡ç®— (åªåŒ…å« 0-8)
        self.digit_canonical_ids = [] # å­˜å‚¨ 0-8 çš„æ ‡å‡† Token IDï¼Œç”¨äºæå– Soft Logits
        
        # --- è¶…å‚æ•°è®¾ç½® ---
        self.key_token_weight = 10.0  # ç¡¬æ ‡ç­¾æƒé‡ (åšå¯¹äº†å¥–åŠ±å¤§)
        self.soft_loss_weight = 2.0   # è½¯æ ‡ç­¾æƒé‡ (æ§åˆ¶è·ç¦»æƒ©ç½šçš„åŠ›åº¦)
        self.sigma = 1.0              # é«˜æ–¯åˆ†å¸ƒæ ‡å‡†å·® (è¶Šå¤§è¶Šå®½å®¹)

        # --- A. æ³¨å†Œæ•°å­— 0-8 (å‚ä¸é«˜æ–¯è®¡ç®—) ---
        for i in range(9):
            s = str(i)
            # è·å–è¯¥æ•°å­—çš„æ‰€æœ‰å¯èƒ½ Token ID (ä¾‹å¦‚ "1", " 1")
            ids = [
                self.tokenizer.convert_tokens_to_ids(s),
                self.tokenizer.convert_tokens_to_ids(" " + s)
            ]
            
            # è®°å½•ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ ID ä½œä¸ºè¯¥æ•°å­—çš„"ä»£è¡¨"ï¼Œç”¨äºæå– Logits è®¡ç®— Soft Loss
            # (é€šå¸¸ tokenizer çš„ç¬¬ä¸€ä¸ªç»“æœå°±æ˜¯æœ€å¸¸ç”¨çš„)
            canonical_added = False
            
            for tid in ids:
                if tid != self.tokenizer.unk_token_id:
                    self.target_token_ids.add(tid)
                    self.id_to_value[tid] = i  # å»ºç«‹ ID -> æ•´æ•°å€¼ çš„æ˜ å°„
                    
                    if not canonical_added:
                        self.digit_canonical_ids.append(tid)
                        canonical_added = True
        
        # ç¡®ä¿æˆ‘ä»¬æ”¶é›†é½äº† 0-8 çš„ä»£è¡¨ IDï¼Œå¦åˆ™æ— æ³•è¿›è¡Œ Softmax è®¡ç®—
        if len(self.digit_canonical_ids) != 9:
            print("âš ï¸ Warning: æ— æ³•æ‰¾åˆ° 0-8 çš„å®Œæ•´ Token IDï¼Œè½¯æ ‡ç­¾é€»è¾‘å¯èƒ½å—æŸã€‚")

        # --- B. æ³¨å†Œè´Ÿå·/-1 (åªåŠ æƒï¼Œä¸å‚ä¸é«˜æ–¯) ---
        # -1 ä»£è¡¨ Stopï¼Œå®ƒåœ¨ç©ºé—´ä¸Šæ²¡æœ‰"é‚»å±…"ï¼Œæ‰€ä»¥åªåšç¡¬åˆ†ç±»
        neg_ids = [
            self.tokenizer.convert_tokens_to_ids("-"),
            self.tokenizer.convert_tokens_to_ids(" -"),
            self.tokenizer.convert_tokens_to_ids("-1"),
            self.tokenizer.convert_tokens_to_ids(" -1")
        ]
        for tid in neg_ids:
            if tid != self.tokenizer.unk_token_id:
                self.target_token_ids.add(tid)
                # æ³¨æ„ï¼šä¸åœ¨ id_to_value ä¸­æ³¨å†Œ

        # æ‰“å°æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if self.is_world_process_zero():
            print(f"WeightedTrainer Ready:")
            print(f"  - Hard Weighted Tokens: {len(self.target_token_ids)}")
            print(f"  - Distance Aware Tokens: 0-8 (Sigma={self.sigma})")

    def generate_gaussian_target(self, gt_values, num_classes=9):
        """
        ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒç›®æ ‡
        gt_values: [Batch] çœŸå®çš„æ•°å­—å€¼ (0-8)
        """
        device = gt_values.device
        # åˆ›å»º [Batch, 9] çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ 0,1,2...8
        target_indices = torch.arange(num_classes, device=device).expand(len(gt_values), -1)
        # æ‰©å±• GT: [Batch, 1] -> [Batch, 9]
        gt_expand = gt_values.unsqueeze(1).expand(-1, num_classes)
        
        # è®¡ç®—è·ç¦»å¹³æ–¹
        distance = (target_indices - gt_expand).float() ** 2
        
        # é«˜æ–¯å…¬å¼: exp(-dist / 2*sigma^2)
        scores = torch.exp(-distance / (2 * self.sigma ** 2))
        
        # å½’ä¸€åŒ– (Sum = 1)ï¼Œè¿™å°±å˜æˆäº†ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ
        probs = scores / scores.sum(dim=1, keepdim=True)
        return probs

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        Loss = Hard_Weighted_CE + Alpha * Soft_Gaussian_KL
        """
        # 1. è·å– Labels
        labels = inputs.get("labels")
        
        # 2. å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # 3. Shift æ“ä½œ (å¯¹é½ Logits å’Œ Labels)
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()

        # 4. å±•å¹³
        batch_size, seq_len, vocab_size = shift_logits.shape
        flat_logits = shift_logits.view(-1, vocab_size)
        flat_labels = shift_labels.view(-1)

        # ==================== Part 1: åŸºç¡€åŠ æƒ Loss (Hard Target) ====================
        # è®¡ç®—æ‰€æœ‰ Token çš„ CrossEntropy
        loss_fct = nn.CrossEntropyLoss(reduction='none', ignore_index=-100)
        token_losses = loss_fct(flat_logits, flat_labels)

        # æ„å»ºæƒé‡çŸ©é˜µ
        weights = torch.ones_like(token_losses)
        
        # æ ‡è®°å“ªäº›ä½ç½®æ˜¯éœ€è¦è®¡ç®—è·ç¦»çš„æ•°å­— (0-8)
        ordinal_mask = torch.zeros_like(token_losses, dtype=torch.bool)
        # å­˜å‚¨è¿™äº›ä½ç½®å¯¹åº”çš„çœŸå®æ•´æ•°å€¼
        ordinal_gt_values = torch.zeros_like(flat_labels, dtype=torch.long)

        # åº”ç”¨æƒé‡å¹¶è¯†åˆ«æ•°å­—
        # (è¿™é‡Œä¸ºäº†ä»£ç æ¸…æ™°ä½¿ç”¨äº†å¾ªç¯ï¼ŒToken åªæœ‰åå‡ ä¸ªï¼Œå¼€é”€å¯å¿½ç•¥)
        for target_id in self.target_token_ids:
            is_target = (flat_labels == target_id)
            # åŠ æƒ
            weights[is_target] = self.key_token_weight
            
            # å¦‚æœæ˜¯ 0-8ï¼ŒåŠ å…¥ Soft Loss è®¡ç®—é˜Ÿåˆ—
            if target_id in self.id_to_value:
                ordinal_mask |= is_target
                # è®°å½•è¯¥ Token ID å¯¹åº”çš„æ•´æ•°å€¼ (ä¾‹å¦‚ ID 299 -> Value 8)
                ordinal_gt_values[is_target] = self.id_to_value[target_id]

        weighted_loss = token_losses * weights
        
        # è®¡ç®—å¹³å‡ Hard Loss
        active_elements = (flat_labels != -100).sum()
        base_loss = weighted_loss.sum() / (active_elements + 1e-6)

        # ==================== Part 2: è·ç¦»æ„ŸçŸ¥ Loss (Soft Target) ====================
        soft_loss = torch.tensor(0.0, device=flat_logits.device)
        
        if ordinal_mask.any():
            # 1. å–å‡ºå±äºæ•°å­—çš„æ ·æœ¬çš„ Logits
            # æˆ‘ä»¬åªå…³å¿ƒæ¨¡å‹åœ¨ 0-8 è¿™ 9 ä¸ª Token ä¸Šçš„è¡¨ç°
            # digit_canonical_ids æ˜¯æˆ‘ä»¬é¢„å…ˆå­˜å¥½çš„ [id_0, id_1, ..., id_8]
            digit_ids_tensor = torch.tensor(self.digit_canonical_ids, device=flat_logits.device)
            
            # æå– Mask å¯¹åº”çš„ Logits è¡Œï¼Œä¸”åªæå– 9 ä¸ªæ•°å­—åˆ— -> [N_ordinal, 9]
            subset_logits = flat_logits[ordinal_mask][:, digit_ids_tensor]
            
            # 2. è®¡ç®— Log Softmax (æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ)
            subset_log_probs = F.log_softmax(subset_logits, dim=-1)
            
            # 3. ç”Ÿæˆé«˜æ–¯ç›®æ ‡åˆ†å¸ƒ (Targetåˆ†å¸ƒ) -> [N_ordinal, 9]
            subset_gt = ordinal_gt_values[ordinal_mask]
            soft_targets = self.generate_gaussian_target(subset_gt, num_classes=9)
            
            # 4. è®¡ç®— KL æ•£åº¦ (KLDiv = -Sum(P_target * log P_pred))
            # è¡¡é‡æ¨¡å‹åˆ†å¸ƒä¸é«˜æ–¯åˆ†å¸ƒçš„å·®å¼‚
            kl_loss = F.kl_div(subset_log_probs, soft_targets, reduction='batchmean')
            
            soft_loss = kl_loss

        # ==================== Part 3: æ€» Loss ====================
        final_loss = base_loss + self.soft_loss_weight * soft_loss

        return (final_loss, outputs) if return_outputs else final_loss

    def save_model(self, output_dir=None, _internal_call=False):
        """
        é‡å†™ä¿å­˜é€»è¾‘ï¼š
        ä¿å­˜ LoRA + Embeddings + Tokenizer
        """
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ä¿å­˜æ¨¡å‹æƒé‡
        super().save_model(output_dir, _internal_call)
        
        # 2. ä¿å­˜ Tokenizer (åªåœ¨ä¸»è¿›ç¨‹)
        if self.is_world_process_zero():
            # å…¼å®¹å¤„ç†: æ–°ç‰ˆ Transformers å»ºè®®ç”¨ processing_class
            saver = getattr(self, "processing_class", None) or getattr(self, "tokenizer", None)
            if saver:
                saver.save_pretrained(output_dir)
                print(f"Model components (Weights + Tokenizer) saved to {output_dir}")

class ClassificationTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.acc_buffer = []
        self.last_eval_visual_step = -1

    def generate_gaussian_target(self, labels, num_classes, sigma=1.0):
        """
        ç”Ÿæˆé«˜æ–¯è½¯æ ‡ç­¾
        labels: [Batch_Size] çœŸå®çš„ç±»åˆ«ç´¢å¼•
        num_classes: æ€»ç±»åˆ«æ•°
        sigma: é«˜æ–¯åˆ†å¸ƒçš„æ ‡å‡†å·®ï¼Œæ§åˆ¶"å®½å®¹åº¦"ã€‚Sigmaè¶Šå¤§ï¼Œå…è®¸çš„è¯¯å·®èŒƒå›´è¶Šå®½ã€‚
        """
        device = labels.device
        batch_size = labels.size(0)
        
        # 1. åˆ›å»ºæ‰€æœ‰ç±»åˆ«çš„ç´¢å¼• [1, num_classes] -> [Batch, num_classes]
        # è¿™é‡Œå‡è®¾ Index 0 æ˜¯ Stopï¼Œä¸å‚ä¸è·ç¦»è®¡ç®—ï¼Œæ‰€ä»¥æˆ‘ä»¬åªå¤„ç† 1~N
        # å¦‚æœä½ çš„ç±»åˆ«å®šä¹‰ä¸åŒï¼Œè¯·ç›¸åº”è°ƒæ•´
        range_tensor = torch.arange(num_classes, device=device).unsqueeze(0).expand(batch_size, -1)
        
        # 2. æ‰©å±•æ ‡ç­¾ç»´åº¦ [Batch, 1]
        target_tensor = labels.unsqueeze(1)
        
        # 3. è®¡ç®—è·ç¦» (ç»å¯¹å€¼è·ç¦»)
        # distance: [Batch, Num_Classes]
        distance = torch.abs(range_tensor - target_tensor)
        
        # --- è¿›é˜¶ï¼šå¦‚æœæ˜¯å…¨æ™¯å›¾(0å’Œ8æ˜¯ç›¸é‚»çš„)ï¼Œå¯ä»¥ä½¿ç”¨ç¯å½¢è·ç¦» ---
        # distance = torch.min(distance, num_classes - 1 - distance) # ä»…å½“é¦–å°¾ç›¸æ¥æ—¶å¼€å¯
        
        # 4. ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒ
        # exp(- dist^2 / (2 * sigma^2))
        scores = torch.exp(- (distance.float() ** 2) / (2 * sigma ** 2))
        
        # 5. ç‰¹æ®Šå¤„ç† Stop æ ‡ç­¾ (Index 0)
        # å‡è®¾ Index 0 æ˜¯ "Stop/åœ"ï¼Œå®ƒä¸åº”è¯¥å’Œ "Index 1 (æ–¹å‘0)" ç›¸è¿‘
        # é€»è¾‘ï¼š
        # - å¦‚æœçœŸå®æ ‡ç­¾æ˜¯ 0: ç›®æ ‡å°±æ˜¯ One-hot [1, 0, 0...]
        # - å¦‚æœçœŸå®æ ‡ç­¾æ˜¯ >0: ç›®æ ‡æ˜¯åœ¨ 1~N ä¹‹é—´çš„é«˜æ–¯åˆ†å¸ƒï¼Œä¸” Index 0 çš„æ¦‚ç‡è®¾ä¸º 0
        
        # åˆ›å»ºä¸€ä¸ª maskï¼Œæ ‡è®°å“ªäº›æ ·æœ¬çš„ GT æ˜¯ 0
        is_stop_token = (labels == 0) # [Batch]
        
        # å¯¹äº GT != 0 çš„æ ·æœ¬ï¼ŒæŠŠ Index 0 çš„æ¦‚ç‡å¼ºåˆ¶è®¾ä¸º 0 (æˆ–è€…æå°å€¼)
        scores[:, 0] = 0.0
        
        # 6. å½’ä¸€åŒ– (è®©æ¦‚ç‡å’Œä¸º 1)
        # åŠ ä¸Š epsilon é˜²æ­¢é™¤é›¶
        probs = scores / (scores.sum(dim=1, keepdim=True) + 1e-9)
        
        # 7. å¯¹äº GT == 0 çš„æ ·æœ¬ï¼Œå¼ºåˆ¶æ¢å¤ä¸º Hard Label [1, 0, 0, ...]
        # æ„é€  One-hot
        one_hot = torch.zeros_like(probs)
        one_hot.scatter_(1, target_tensor, 1.0)
        
        # ç»„åˆï¼šå¦‚æœæ˜¯ Stop åˆ™ç”¨ One-hotï¼Œå¦åˆ™ç”¨é«˜æ–¯åˆ†å¸ƒ
        final_targets = torch.where(is_stop_token.unsqueeze(1), one_hot, probs)
        
        return final_targets

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        Loss è®¡ç®— (é«˜æ–¯è½¯æ ‡ç­¾ç‰ˆ) + å‡†ç¡®ç‡ç´¯ç§¯
        """
        labels = inputs.get("class_labels")
        if labels is None:
            labels = inputs.get("labels")
            
        outputs = model(**inputs)
        logits = outputs.get("logits") # [Batch, Num_Classes]
        
        loss = None
        if logits is not None:
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ Soft Target Cross Entropy ---
            
            # 1. ç”Ÿæˆè½¯æ ‡ç­¾ç›®æ ‡
            # sigma=1.0 è¡¨ç¤ºç›¸é‚»çš„ 1 ä¸ªå•ä½ Loss ä¹Ÿå¾ˆå°
            # sigma=0.5 è¡¨ç¤ºè¦æ±‚æ¯”è¾ƒä¸¥æ ¼
            # sigma=2.0 è¡¨ç¤ºéå¸¸å®½å®¹
            num_classes = logits.size(-1)
            soft_targets = self.generate_gaussian_target(labels, num_classes, sigma=1.5)
            
            # 2. è®¡ç®— Loss
            # CrossEntropyLoss(pred, soft_target) ç­‰ä»·äº -sum(target * log_softmax(pred))
            log_probs = F.log_softmax(logits, dim=-1)
            
            # æ ·æœ¬ç»´åº¦çš„ Loss: [Batch]
            # å…¬å¼: KL Divergence (å¿½ç•¥å¸¸æ•°é¡¹) -> Cross Entropy
            loss_per_sample = -torch.sum(soft_targets * log_probs, dim=-1)
            loss = loss_per_sample.mean()

        # 4. è®¡ç®—å‡†ç¡®ç‡ (ä¿æŒä¸å˜ï¼Œå‡†ç¡®ç‡è¿˜æ˜¯çœ‹ç¡¬æŒ‡æ ‡)
        if logits is not None:
            with torch.no_grad():
                preds = torch.argmax(logits, dim=-1)
                micro_acc = (preds == labels).float().mean().item()
                
                if model.training:
                    self.acc_buffer.append(micro_acc)
                    if len(self.acc_buffer) >= self.args.gradient_accumulation_steps:
                        avg_acc = sum(self.acc_buffer) / len(self.acc_buffer)
                        self.log({"train/accuracy": avg_acc})
                        self.acc_buffer = []

        self._handle_visualization(model, inputs, preds, labels)
        return (loss, outputs) if return_outputs else loss
    def _handle_visualization(self, model, inputs, preds, labels):
        """
        æ§åˆ¶ä½•æ—¶æˆªå›¾å¹¶ä¸Šä¼ åˆ° SwanLab
        """
        current_step = self.state.global_step

        # æƒ…å†µ 1: æ­£åœ¨è®­ç»ƒ (Training)
        # æ¯ 50 æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒé›†çš„å›¾
        if model.training:
            if self.is_world_process_zero() and current_step % 50 == 0:
                self._log_visuals(inputs, preds, labels, prefix="Train")

        # æƒ…å†µ 2: æ­£åœ¨éªŒè¯ (Evaluation)
        # model.training ä¸º False
        else:
            # åªæœ‰åœ¨ä¸»è¿›ç¨‹ï¼Œä¸”å½“å‰è¿™ä¸€è½® Eval è¿˜æ²¡è®°å½•è¿‡å›¾ç‰‡æ—¶ï¼Œæ‰è®°å½•
            # (Trainer åœ¨ Eval è¿‡ç¨‹ä¸­ global_step æ˜¯ä¸ä¼šå˜çš„)
            if self.is_world_process_zero() and self.last_eval_visual_step != current_step:
                self._log_visuals(inputs, preds, labels, prefix="Eval")
                # æ ‡è®°è¿™ä¸€è½®å·²ç»è®°å½•è¿‡äº†
                self.last_eval_visual_step = current_step

    def _log_visuals(self, inputs, preds, labels, prefix="Train"):
        """
        æ‰§è¡Œå…·ä½“çš„ä¸Šä¼ æ“ä½œ
        prefix: ç”¨äºåŒºåˆ†æ˜¯ 'Train' è¿˜æ˜¯ 'Eval'
        """
        try:
            idx = 0 # å– Batch ç¬¬ä¸€å¼ å›¾
            
            # 1. è¿˜åŸæ–‡æœ¬
            instruction_text = self.processing_class.decode(
                inputs["input_ids"][idx], 
                skip_special_tokens=True
            )
            display_text = instruction_text[:100] + "..." if len(instruction_text) > 100 else instruction_text

            # 2. è¿˜åŸå›¾ç‰‡
            # [Batch, 5, 3, H, W] -> å–æœ€åä¸€å¸§ -> [3, H, W]
            rgb_tensor = inputs["pixel_values"][idx][-1] 
            rgb_img = self._tensor_to_pil(rgb_tensor)

            depth_tensor = inputs["depth_pixel_values"][idx][-1]
            depth_img = self._tensor_to_pil(depth_tensor, is_depth=True)

            # 3. æ„å»º Caption
            pred_val = preds[idx].item() - 1  # è¿˜åŸå› -1~8
            gt_val = labels[idx].item() - 1
            status = "âœ…" if pred_val == gt_val else "âŒ"
            
            caption = (f"[{prefix}] {status} Pred: {pred_val} | GT: {gt_val}\n"
                       f"{display_text}")

            # 4. å‘é€ SwanLab (ä½¿ç”¨ prefix åˆ†ç»„)
            swanlab.log({
                f"Visual/{prefix}_RGB": swanlab.Image(rgb_img, caption=caption),
                f"Visual/{prefix}_Depth": swanlab.Image(depth_img, caption="Depth Map")
            })
            
        except Exception as e:
            print(f"SwanLab Visual Error: {e}")

    def _tensor_to_pil(self, tensor, is_depth=False):
        """åå½’ä¸€åŒ–å¹¶è½¬ PIL"""
        img = tensor.cpu().numpy().transpose(1, 2, 0)
        img = img - img.min()
        img = img / (img.max() + 1e-6)
        img = (img * 255).astype(np.uint8)
        return img
    def save_model(self, output_dir=None, _internal_call=False):
        """
        é™¤äº†ä¿å­˜ LoRA æƒé‡å¤–ï¼Œå¼ºåˆ¶ä¿å­˜ Tokenizer å’Œ Embeddings (å¦‚æœåœ¨ modules_to_save ä¸­)
        """
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. è°ƒç”¨çˆ¶ç±»æ–¹æ³•ä¿å­˜æ¨¡å‹æƒé‡ (LoRA + modules_to_save)
        super().save_model(output_dir, _internal_call)
        
        # 2. é¢å¤–ä¿å­˜ Tokenizer (åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ)
        if self.is_world_process_zero():
            # å…¼å®¹æ–°ç‰ˆ transformers ä½¿ç”¨ processing_classï¼Œæ—§ç‰ˆä½¿ç”¨ tokenizer
            if hasattr(self, "processing_class") and self.processing_class is not None:
                self.processing_class.save_pretrained(output_dir)
            elif hasattr(self, "tokenizer") and self.tokenizer is not None:
                self.tokenizer.save_pretrained(output_dir)
            
            print(f"Model (LoRA + Embeddings + Tokenizer) saved to {output_dir}")



def main():
    # =================Configuration=================
    model_name_or_path = "./instructblip-vicuna-7b" 
    # Weight: Fusion, Q-Former, Depth
    stage1_checkpoint = "checkpoint/latest_checkpoint.pth"
    data_path = "dataset_waypoint/rgb_images_r2r_train_processed.json"
    output_dir = "./output/rvln_sft_llm"
    # è®­ç»ƒå‚æ•°
    batch_size = 2
    grad_accumulation = 8 # ç¨å¾®åŠ å¤§ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    learning_rate = 5e-5  # SFT LLM å­¦ä¹ ç‡
    num_epochs = 3
    lora_rank = 32
    lora_alpha = 64
    
    # ================= [SwanLab] 2. åˆå§‹åŒ– SwanLab =================
    # åœ¨è¿™é‡Œå®šä¹‰å®éªŒåç§°å’Œéœ€è¦è®°å½•çš„é…ç½®ä¿¡æ¯
    swanlab.login(api_key="jpnq84pFWGxNf9thDyX9P")
    swanlab.init(
        project="Rvln-LoRA-SFT",
        experiment_name="vicuna-7b-lora-stage2",
        description="Rvln Stage 2 SFT with LoRA monitoring",
        config={
            "model_name": model_name_or_path,
            "stage1_checkpoint": stage1_checkpoint,
            "data_path": data_path,
            "batch_size": batch_size,
            "grad_accumulation": grad_accumulation,
            "learning_rate": learning_rate,
            "num_epochs": num_epochs,
            "lora_rank": lora_rank,
            "lora_alpha": lora_alpha,
            "lora_dropout": 0.05,
            "modules_to_save": ["embed_tokens", "lm_head"]# "score_head"
        }
    )
    
    # =================1. Processor & Tokenizer=================
    print("Loading Processor...")
    processor = InstructBlipProcessor.from_pretrained(model_name_or_path)
    tokenizer = processor.tokenizer
    qformer_tokenizer = processor.qformer_tokenizer
    tokenizer.padding_side = "right"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # æ·»åŠ ç‰¹æ®Š Token
    special_tokens_dict = {'additional_special_tokens': ["<history>", "<current>"]}
    tokenizer.add_special_tokens(special_tokens_dict)
    
    history_token_id = tokenizer.convert_tokens_to_ids("<history>")
    current_token_id = tokenizer.convert_tokens_to_ids("<current>")

    # =================2. Model Initialization=================
    print("Loading Base Model...")
    config = InstructBlipConfig.from_pretrained(model_name_or_path)
    config.history_token_id = history_token_id
    config.current_token_id = current_token_id

    # åŠ è½½åŸºç¡€æ¨¡å‹
    model = RvlnMultiTask.from_pretrained(
        model_name_or_path,
        config=config,
        torch_dtype=torch.float16
    )

    # è°ƒæ•´ Embedding å¤§å°
    model.language_model.resize_token_embeddings(len(tokenizer))

    # =================3. [å…³é”®] åŠ è½½ Stage 1 è®­ç»ƒå¥½çš„æƒé‡=================
    if os.path.exists(stage1_checkpoint):
        print(f"ğŸ“¥ Loading Stage 1 Checkpoint from: {stage1_checkpoint}")
        ckpt = torch.load(stage1_checkpoint, map_location="cpu")
        
        msg = model.load_state_dict(ckpt, strict=False) 
        print(f"Checkpoint Load Status: {msg}")
        
        if 'visual_fusion' in ckpt: print(" - Visual Fusion Loaded âœ…")
        if 'qformer' in ckpt: print(" - Q-Former Loaded âœ…")
        if 'depth_backbone' in ckpt: print(" - Depth Backbone Loaded âœ…")
    else:
        print("âŒ Warning: Stage 1 checkpoint not found! Training from scratch (Not Recommended).")

    # =================4. Freeze & LoRA Setup=================
    
    # 4.1 å…¨å±€å†»ç»“
    for param in model.parameters():
        param.requires_grad = False
        
    # 4.2 é…ç½® LoRA (é’ˆå¯¹ LLM)
    peft_config = LoraConfig(
        r=lora_rank,
        lora_alpha=lora_alpha,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        modules_to_save=["embed_tokens", "lm_head"]# "score_head" 
    )
    
    print("Applying LoRA to LLM...")
    model.language_model = get_peft_model(model.language_model, peft_config)
    
    print_trainable_parameters(model)

# ================= 5. Data Setup (å…³é”®ä¿®æ”¹ï¼šåˆ’åˆ†éªŒè¯é›†) =================
    print("Loading Full Dataset...")
    full_dataset = RvlnLoRADataset(
        data_path=data_path,
        processor=processor,
        tokenizer=tokenizer,
        image_root="", 
        history_len=4,
        current_len=1
    )
    
    val_ratio = 0.01  # 1% åšéªŒè¯ï¼Œ99% è®­ç»ƒ
    val_size = int(len(full_dataset) * val_ratio)
    train_size = len(full_dataset) - val_size
    
    print(f"Splitting Dataset: Total={len(full_dataset)} | Train={train_size} | Val={val_size}")
    
    # generatorç”¨äºå›ºå®šéšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡åˆ‡åˆ†ä¸€æ ·ï¼Œæ–¹ä¾¿å¤ç°
    train_dataset, eval_dataset = random_split(
        full_dataset, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42) 
    )

    collator = DataCollatorForRvln(
        processor=processor,
        tokenizer=tokenizer,
        qformer_tokenizer=qformer_tokenizer
    )

    # ================= 6. Trainer Setup (å…³é”®ä¿®æ”¹ï¼šæ·»åŠ  Eval é…ç½®) =================
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=grad_accumulation,
        learning_rate=learning_rate,
        num_train_epochs=num_epochs,
        fp16=True,
        deepspeed="./config/ds_config_zero2.json",
        remove_unused_columns=False,
        report_to="none",
        evaluation_strategy="steps",   # æŒ‰æ­¥æ•°è¯„ä¼° (ä¹Ÿå¯ä»¥é€‰ "epoch")
        eval_steps=1000,                # æ¯ 100 æ­¥è¯„ä¼°ä¸€æ¬¡éªŒè¯é›† (æ ¹æ®ä½ æ€»æ­¥æ•°è°ƒæ•´)
        per_device_eval_batch_size=batch_size, # éªŒè¯é›†çš„ Batch Size
        save_strategy="steps",         # å¿…é¡»å’Œ evaluation_strategy ä¸€è‡´
        save_steps=2000,                # æ¯ 2000 æ­¥å°è¯•ä¿å­˜
        save_total_limit=2,            # æœ€å¤šä¿ç•™ 2 ä¸ª checkpointï¼Œçœç¡¬ç›˜
        load_best_model_at_end=True,   # è®­ç»ƒç»“æŸæ—¶ï¼Œè‡ªåŠ¨åŠ è½½éªŒè¯é›†æ•ˆæœæœ€å¥½çš„æ¨¡å‹
        metric_for_best_model="loss",  # ä»¥ loss ä¸ºæ ‡å‡† (loss è¶Šå°è¶Šå¥½)
        greater_is_better=False,       # loss æ˜¯è¶Šå°è¶Šå¥½ï¼Œæ‰€ä»¥æ˜¯ False
        logging_steps=4,
    )

    # ä½¿ç”¨è‡ªå®šä¹‰ Trainer
    trainer = WeightedTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=collator,
        processing_class=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[SwanLabCallback()],
        preprocess_logits_for_metrics=preprocess_logits_for_metrics
    )

    trainer.train()
    trainer.accelerator.wait_for_everyone()
    
    # ä»…ç”±ä¸»è¿›ç¨‹è§¦å‘ä¿å­˜é€»è¾‘ï¼ˆæˆ–è€… trainer.save_model å†…éƒ¨ä¼šå¤„ç†ï¼Œä½†åŠ ä¸Š wait æ›´å®‰å…¨ï¼‰
    if trainer.is_world_process_zero():
        trainer.save_model(output_dir)
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == "__main__":
    main()
