import os
import torch
from transformers import InstructBlipProcessor, InstructBlipConfig
from peft import PeftModel
from models.WayPointVLN import RvlnMultiTask  # ç¡®ä¿èƒ½å¯¼å…¥ä½ çš„è‡ªå®šä¹‰æ¨¡å‹ç±»

def merge_lora():
    # ================= é…ç½®è·¯å¾„ =================
    base_model_path = "./instructblip-vicuna-7b"
    adapter_path = "./output_116/final_adapter" 
    stage1_weights_path = "output/stage1_checkpoint/latest_checkpoint.pth"
    depth_encoder_path = "./vit-base-patch16-224"

    output_path = "./output/rvln_merged_final_116"
    
    print(f"ğŸš€ å¼€å§‹åˆå¹¶æµç¨‹...")
    print(f" -> Base: {base_model_path}")
    print(f" -> Adapter: {adapter_path}")
    print(f" -> Output: {output_path}")

    # ================= 1. åŠ è½½ Processor å’Œ Config =================
    print("â³ Loading Processor & Config...")
    processor = InstructBlipProcessor.from_pretrained(base_model_path)    
    tokenizer = processor.tokenizer
    
    special_tokens_dict = {'additional_special_tokens': ["<history>", "<current>"]}
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
    print(f" -> Added {num_added_toks} special tokens. Vocab size: {len(tokenizer)}")

    config = InstructBlipConfig.from_pretrained(base_model_path)
    config.history_token_id = tokenizer.convert_tokens_to_ids("<history>")
    config.current_token_id = tokenizer.convert_tokens_to_ids("<current>")
    config.depth_model_name_or_path = depth_encoder_path
    print("â³ Loading Base Model (RvlnMultiTask)...")
    model = RvlnMultiTask.from_pretrained(
        base_model_path,
        config=config,
        torch_dtype=torch.float16 # å»ºè®®ç”¨ float16 èŠ‚çœæ˜¾å­˜
    )
    model.language_model.resize_token_embeddings(len(tokenizer))
    config.vocab_size = len(tokenizer)
    # ================= 3. [å…³é”®] åŠ è½½ Stage 1 è§†è§‰æƒé‡ =================
    # æˆ‘ä»¬å¸Œæœ›æœ€ç»ˆä¿å­˜çš„æ¨¡å‹åŒ…å«ï¼š[åŸå§‹ViT] + [è®­ç»ƒå¥½çš„Fusion/Q-Former] + [èåˆäº†LoRAçš„LLM]
    # æ‰€ä»¥åœ¨åˆå¹¶ LLM ä¹‹å‰ï¼Œå…ˆæŠŠè§†è§‰éƒ¨åˆ†æ›´æ–°åˆ°æœ€æ–°çŠ¶æ€
    if os.path.exists(stage1_weights_path):
        print(f"ğŸ“¥ Loading Stage 1 Visual Weights from: {stage1_weights_path}")
        stage1_state_dict = torch.load(stage1_weights_path, map_location="cpu")
        msg = model.load_state_dict(stage1_state_dict, strict=False)
        print(f"   Load Status: {msg}")
    else:
        print("âš ï¸ Warning: Stage 1 weights not found! The visual part will remain original/random.")

    # ================= 4. åŠ è½½ LoRA Adapter =================
    print("â³ Loading LoRA Adapter...")
    # ä½ çš„ LoRA æ˜¯åŠ åœ¨ language_model ä¸Šçš„
    # æ‰€ä»¥æˆ‘ä»¬è¦æŠŠ adapter æŒ‚è½½åˆ° model.language_model ä¸Š
    
    # å…³é”®ï¼šPeftModel.from_pretrained ä¼šè‡ªåŠ¨è¯†åˆ« adapter_config.json
    model.language_model = PeftModel.from_pretrained(
        model.language_model,
        adapter_path,
        torch_dtype=torch.float16
    )

    # ================= 5. æ‰§è¡Œåˆå¹¶ (Merge & Unload) =================
    print("âš¡ Merging LoRA into Base Model...")
    # è¿™ä¸€æ­¥ä¼šæŠŠ LoRA çš„çŸ©é˜µ A*B åŠ å›åˆ°åŸå§‹æƒé‡ W ä¸Šï¼Œå¹¶ç§»é™¤ LoRA å±‚
    model.language_model = model.language_model.merge_and_unload()
    
    # éªŒè¯ä¸€ä¸‹ï¼šç°åœ¨ model.language_model åº”è¯¥å˜å›äº†åŸæ¥çš„ LlamaForCausalLM (æˆ–ç±»ä¼¼)ï¼Œä¸å†æ˜¯ PeftModel

    # ================= 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ =================
    print(f"ğŸ’¾ Saving Merged Model to: {output_path} ...")
    os.makedirs(output_path, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹æƒé‡ (è¿™ä¼šä¿å­˜æ•´ä¸ª RvlnMultiTaskï¼ŒåŒ…å«è§†è§‰éƒ¨åˆ†å’Œèåˆåçš„ LLM)
    model.save_pretrained(output_path)
    
    # ä¿å­˜ Processor / Tokenizer
    processor.save_pretrained(output_path)
    
    # å¦‚æœä½ æœ‰è‡ªå®šä¹‰ tokenizer æ–‡ä»¶åœ¨ adapter ç›®å½•é‡Œï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶è¿‡å»
    # tokenizer.save_pretrained(output_path)

    print("âœ… Merge Complete! You can now use the model directly without loading adapters.")

if __name__ == "__main__":
    merge_lora()