import torch
import os
import glob
import gc
from safetensors.torch import load_file
from peft import LoraConfig, get_peft_model
from models.rvln import RvlnMultiTask
from transformers import InstructBlipConfig, InstructBlipProcessor

# ================= é…ç½®åŒºåŸŸ =================
sharded_weights_dir = "lora_weight/rvln_sft_llm" 
base_model_path = "instructblip-vicuna-7b"
# 3. æœ€ç»ˆè¾“å‡ºè·¯å¾„ (åˆå¹¶åï¼Œå¯ä»¥ç›´æ¥ from_pretrained çš„ç›®å½•)
output_dir = "./output/rvln_merged_final_1"

# 4. LoRA é…ç½® (å¿…é¡»æ‰‹åŠ¨é‡å»ºï¼Œå› ä¸º adapter_config ä¸¢äº†)
# è¯·ç¡®ä¿è¿™äº›å‚æ•°å’Œä½ è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´
lora_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 5. è¾“å‡ºåˆ†ç‰‡å¤§å° (æ§åˆ¶è¾“å‡ºæ–‡ä»¶çš„æ•°é‡)
# Vicuna-7B çº¦ 13GBã€‚è®¾ç½® 4GB å·¦å³å¤§çº¦ä¼šç”Ÿæˆ 3-4 ä¸ªæ–‡ä»¶ã€‚
SHARD_SIZE = "4GB" 
# ===========================================

def main():
    print(f"ğŸš€ å¼€å§‹åˆå¹¶ä»»åŠ¡...")
    print(f"   è¾“å…¥ç›®å½•: {sharded_weights_dir}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")

    # ---------------------------------------------------------
    # Step 1: æ­å»ºå¸¦æœ‰ LoRA æ’æ§½çš„æ¨¡å‹éª¨æ¶
    # ---------------------------------------------------------
    print("\n[1/5] åˆå§‹åŒ–æ¨¡å‹éª¨æ¶...")
    config = InstructBlipConfig.from_pretrained(base_model_path)
    
    # âš ï¸ å¦‚æœä½ æœ‰è‡ªå®šä¹‰ token (å¦‚ <history>), è¯·ç¡®ä¿ config é‡Œå·²æ›´æ–°
    # æœ€å¥½æ˜¯ä» sharded_weights_dir é‡Œè¯»å– config.json (å¦‚æœæœ‰çš„è¯)
    if os.path.exists(os.path.join(sharded_weights_dir, "config.json")):
        print("   -> å‘ç°æ–° config.jsonï¼Œä½¿ç”¨æ–°é…ç½®")
        config = InstructBlipConfig.from_pretrained(sharded_weights_dir)
    
    # ä½¿ç”¨ CPU åŠ è½½ä»¥èŠ‚çœæ˜¾å­˜ (7B æ¨¡å‹çº¦éœ€ 14GB RAM)
    model = RvlnMultiTask.from_pretrained(
        base_model_path,
        config=config,
        torch_dtype=torch.float16,
        device_map="cpu"
    )

    # æŒ‚è½½ LoRA ç©ºå£³ (å…³é”®ï¼šè¿™ä¼šåˆ›å»º lora_A/lora_B çš„å±‚ï¼Œè®©æƒé‡æœ‰åœ°æ–¹æ”¾)
    print("   -> æŒ‚è½½ LoRA ç»“æ„...")
    model.language_model = get_peft_model(model.language_model, lora_config)


    # ---------------------------------------------------------
    # Step 2: è¯»å–æ‰€æœ‰åˆ†ç‰‡æƒé‡åˆ°å†…å­˜
    # ---------------------------------------------------------
    print("\n[2/5] è¯»å–åˆ†ç‰‡æƒé‡...")
    shard_files = sorted(glob.glob(os.path.join(sharded_weights_dir, "*.safetensors")))
    if not shard_files:
        raise FileNotFoundError("æœªæ‰¾åˆ° .safetensors æ–‡ä»¶")

    full_state_dict = {}
    for i, shard in enumerate(shard_files):
        print(f"   -> Loading shard {i+1}/{len(shard_files)}: {os.path.basename(shard)} ...")
        # å¼ºåˆ¶åŠ è½½åˆ° CPU
        shard_weights = load_file(shard, device="cpu")
        full_state_dict.update(shard_weights)
        del shard_weights # é‡Šæ”¾å†…å­˜
        gc.collect()

    print(f"   -> æ€»è®¡åŠ è½½ Key æ•°é‡: {len(full_state_dict)}")


    # ---------------------------------------------------------
    # Step 3: æƒé‡æ³¨å…¥
    # ---------------------------------------------------------
    print("\n[3/5] å°†æƒé‡æ³¨å…¥æ¨¡å‹...")
    # strict=False å…è®¸å¿½ç•¥ä¸€äº›æ— å…³ç´§è¦çš„ buffer
    missing, unexpected = model.load_state_dict(full_state_dict, strict=False)
    
    # é‡Šæ”¾å·¨å¤§çš„ state_dict å­—å…¸ï¼Œè…¾å‡ºå†…å­˜ç»™ä¸‹ä¸€æ­¥ Merge
    del full_state_dict
    gc.collect()

    if len(unexpected) > 0:
        print(f"   âš ï¸ Warning: Unexpected keys (å‰3ä¸ª): {unexpected[:3]}")
    else:
        print("   âœ… æƒé‡åŠ è½½å®Œç¾åŒ¹é…ã€‚")


    # ---------------------------------------------------------
    # Step 4: æ‰§è¡Œåˆå¹¶ (Merge)
    # ---------------------------------------------------------
    print("\n[4/5] æ‰§è¡Œ Merge & Unload...")
    # è¿™ä¸€æ­¥å°† (Base + LoRA) æ°¸ä¹…åˆå¹¶ä¸ºä¸€ä¸ªæ™®é€šçš„ Linear çŸ©é˜µ
    model.language_model = model.language_model.merge_and_unload()
    
    # éªŒè¯æ¨¡å‹ç°åœ¨æ˜¯å¦è¿˜æ˜¯ PeftModel
    print(f"   -> å½“å‰æ¨¡å‹ç±»å‹: {type(model.language_model)}")
    # æ­¤æ—¶åº”è¯¥å·²ç»å˜å›äº†æ™®é€šçš„ LlamaForCausalLM æˆ–ç±»ä¼¼ç»“æ„


    # ---------------------------------------------------------
    # Step 5: é‡æ–°åˆ‡åˆ†å¹¶ä¿å­˜
    # ---------------------------------------------------------
    print(f"\n[5/5] ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {output_dir}...")
    os.makedirs(output_dir, exist_ok=True)

    # max_shard_size ä¼šè‡ªåŠ¨å¸®ä½ åˆ‡åˆ†æ–‡ä»¶
    model.save_pretrained(
        output_dir, 
        max_shard_size=SHARD_SIZE, 
        safe_serialization=True
    )
    
    # åˆ«å¿˜äº†ä¿å­˜ Tokenizer/Processorï¼Œæ–¹ä¾¿åç»­ç›´æ¥ç”¨
    try:
        print("   -> å¤åˆ¶ Processor/Tokenizer...")
        processor = InstructBlipProcessor.from_pretrained(base_model_path)
        # å¦‚æœä½ ä¹‹å‰æ·»åŠ äº†ç‰¹æ®Š tokenï¼Œè®°å¾—åœ¨è¿™é‡Œ add_special_tokens å¹¶ save
        # ...
        processor.save_pretrained(output_dir)
    except Exception as e:
        print(f"   âš ï¸ Processor ä¿å­˜å¤±è´¥ (å¯èƒ½éœ€è¦æ‰‹åŠ¨å¤åˆ¶): {e}")

    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ç°åœ¨ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨: model = RvlnMultiTask.from_pretrained('{output_dir}')")

if __name__ == "__main__":
    main()