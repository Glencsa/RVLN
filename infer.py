import os 
import torch
import torch.nn as nn
from PIL import Image
import requests
from io import BytesIO
import numpy as np # æ–°å¢ numpy ç”¨äºæ£€æŸ¥ NaN

# å¼•å…¥å®šä¹‰å¥½çš„æ¨¡å‹ç±»
from InstructBlip import InstructBlipMultiTask
from transformers import (
    InstructBlipProcessor,
    BertTokenizer
)

def run_inference():
    # =================================================
    # 1. é…ç½®å‚æ•°
    # =================================================
    MODEL_ID = "./instructblip-vicuna-7b"
    CHECKPOINT_PATH = "./checkpoints_itm_cross_attn/best_checkpoint.pth" 
    
    # å»ºè®®ä½¿ç”¨ cuda
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ã€ä¿®æ”¹ 1ã€‘ä¸»æ¨¡å‹ä½¿ç”¨ bfloat16ï¼Œä½†æ·±åº¦éƒ¨åˆ†æˆ‘ä»¬å°†å¼ºåˆ¶ä½¿ç”¨ float32
    DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

    print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ– (Device: {DEVICE}, Main Dtype: {DTYPE})...")

    # =================================================
    # 2. åŠ è½½å¤„ç†å™¨å’Œ Tokenizer
    # =================================================
    processor = InstructBlipProcessor.from_pretrained(MODEL_ID)
    qformer_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # =================================================
    # 3. åŠ è½½åŸºç¡€æ¨¡å‹ç»“æ„
    # =================================================
    print("ğŸ“¦ åŠ è½½ InstructBlipMultiTask åŸºç¡€æ¨¡å‹...")
    
    # ã€ä¿®æ”¹ 2ã€‘ç§»é™¤ device_map="auto"
    # åŸå› ï¼šä¹‹å‰çš„ "no-op" è­¦å‘Šæ˜¯å› ä¸º meta device åˆå§‹åŒ–å¤±è´¥ã€‚
    # æˆ‘ä»¬å…ˆåŠ è½½åˆ° CPUï¼Œç„¶åæ‰‹åŠ¨æ¬è¿ï¼Œè¿™æ ·èƒ½ç¡®ä¿æ‰€æœ‰å±‚éƒ½è¢«å®å®åœ¨åœ¨åˆ†é…äº†å†…å­˜ã€‚
    # 7B æ¨¡å‹ fp16 å¤§çº¦å ç”¨ 14GB æ˜¾å­˜ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤ŸæŠ¥é”™ OOMï¼Œè¯·å‘Šè¯‰æˆ‘ä¸å¤Ÿï¼Œæˆ‘ä»¬å†æ¢å› auto å¹¶åŠ ç‰¹æ®Šå¤„ç†ã€‚
    model = InstructBlipMultiTask.from_pretrained(
        MODEL_ID, 
        torch_dtype=DTYPE
    )
    
    # æ¨¡å‹ç§»åŠ¨åˆ° GPU
    model.to(DEVICE)

    # =================================================
    # 4. ã€æ ¸å¿ƒä¿®å¤ã€‘è§£å†³ NaN é—®é¢˜
    # =================================================
    print("ğŸ”§ æ­£åœ¨ä¼˜åŒ–ç²¾åº¦é…ç½®ä»¥é¿å… NaN...")
    # ã€ä¿®æ”¹ 3ã€‘å¼ºåˆ¶ Depth Backbone ä½¿ç”¨ float32
    # Vision Backbone åœ¨ LayerNorm è®¡ç®—æ—¶ï¼Œå¦‚æœç”¨ bfloat16 ææ˜“æº¢å‡ºå¯¼è‡´ NaN
    if hasattr(model, 'depth_model'):
        model.depth_model.to(dtype=torch.float32)
        print("   âœ… Depth Model å·²å¼ºåˆ¶è½¬æ¢ä¸º Float32")
    else:
        print("   âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° depth_modelï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®šä¹‰ï¼")

    # =================================================
    # 5. åŠ è½½è®­ç»ƒå¥½çš„ Fusion å’Œ ITM æƒé‡
    # =================================================
    if os.path.exists(CHECKPOINT_PATH):
        print(f"ğŸ“¥ å‘ç°è®­ç»ƒæƒé‡: {CHECKPOINT_PATH}ï¼Œæ­£åœ¨åŠ è½½...")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location="cpu") # å…ˆåŠ è½½åˆ° CPU é˜²æ­¢æ˜¾å­˜æ³¢åŠ¨
        
        # 2. åŠ è½½ Visual Fusion
        try:
            # åŠ è½½æƒé‡
            model.visual_fusion.load_state_dict(checkpoint['visual_fusion'], strict=True)
            # ã€ä¿®æ”¹ 4ã€‘åŠ è½½åï¼Œç¡®ä¿ visual_fusion å›åˆ°æ­£ç¡®çš„è®¾å¤‡å’Œç²¾åº¦
            # æ³¨æ„ï¼šå¦‚æœä½ çš„ fusion å†…éƒ¨æœ‰ LayerNormï¼Œå»ºè®®ä¹Ÿç”¨ float32ï¼Œå¦‚æœæ²¡æœ‰ï¼Œç”¨ DTYPE å³å¯
            model.visual_fusion.to(device=DEVICE, dtype=DTYPE) 
            print(f"   âœ… Visual Fusion åŠ è½½æˆåŠŸ")
        except KeyError:
            print("   âŒ é”™è¯¯: Checkpoint ä¸­æ‰¾ä¸åˆ° 'visual_fusion'ï¼")
        except Exception as e:
            print(f"   âŒ Visual Fusion åŠ è½½æŠ¥é”™: {e}")
            
        # 3. åŠ è½½ ITM Head
        try:
            model.itm_head.load_state_dict(checkpoint['itm_head'], strict=True)
            model.itm_head.to(device=DEVICE, dtype=DTYPE)
            print(f"   âœ… ITM Head åŠ è½½æˆåŠŸ")
        except KeyError:
            print("   âŒ é”™è¯¯: Checkpoint ä¸­æ‰¾ä¸åˆ° 'itm_head'ï¼")
        
    else:
        print(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {CHECKPOINT_PATH}")

    model.eval()

    # =================================================
    # 6. å‡†å¤‡æµ‹è¯•æ•°æ®
    # =================================================
    print("\nğŸ–¼ï¸ å‡†å¤‡æµ‹è¯•å›¾ç‰‡...")
    img_path = "test.jpeg"
    
    if not os.path.exists(img_path):
        url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        print(f"   æœ¬åœ°æ— å›¾ç‰‡ï¼Œæ­£åœ¨ä¸‹è½½ç¤ºä¾‹å›¾ç‰‡: {url}")
        raw_image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
        raw_image.save("test.jpeg")
    else:
        raw_image = Image.open(img_path).convert("RGB")

    # =================================================
    # ä»»åŠ¡ä¸€ï¼šè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ
    # =================================================
    print("\n" + "="*40)
    print("ğŸ§ª æµ‹è¯• 1: è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ")
    print("="*40)
    
    prompt = "Describe this image in detail."
    inputs_gen = processor(images=raw_image, text=prompt, return_tensors="pt").to(DEVICE)
    inputs_gen["pixel_values"] = inputs_gen["pixel_values"].to(dtype=DTYPE)
    
    with torch.no_grad():
        outputs = model.generate(**inputs_gen, max_new_tokens=50)
    
    print(f"Prompt: {prompt}")
    print(f"Output: {processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()}")

    # =================================================
    # ä»»åŠ¡äºŒï¼šå›¾æ–‡åŒ¹é… (ITM) - RGB + Depth Fusion
    # =================================================
    print("\n" + "="*40)
    print("ğŸ§ª æµ‹è¯• 2: å›¾æ–‡åŒ¹é… (ITM)")
    print("="*40)
    
    test_texts = [
        "Two cats sleeping on a pink blanket", 
        "A red sports car driving on the highway", 
        "A man is taking a photo on the beach" 
    ]
    
    image_inputs = processor(images=raw_image, return_tensors="pt").to(DEVICE)
    pixel_values = image_inputs.pixel_values.to(dtype=DTYPE) 
    
    # ã€ä¿®æ”¹ 5ã€‘è¾“å…¥æ•°æ®å®‰å…¨æ£€æŸ¥
    if torch.isnan(pixel_values).any():
        print("âŒ è‡´å‘½é”™è¯¯: è¾“å…¥å›¾åƒ Tensor åŒ…å« NaNï¼")
        return

    text_inputs = qformer_tokenizer(
        test_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32
    ).to(DEVICE)
    
    pixel_values_expanded = pixel_values.repeat(len(test_texts), 1, 1, 1)

    print("æ­£åœ¨è®¡ç®— Cross-Attention Fusion åŠ ITM åˆ†æ•°...")
    with torch.no_grad():
        logits = model.forward_itm(
            pixel_values=pixel_values_expanded,
            input_ids=text_inputs.input_ids,
            attention_mask=text_inputs.attention_mask
        )
        probs = torch.softmax(logits, dim=1)
    
    print("\nğŸ“Š åŒ¹é…ç»“æœ:")
    for i, text in enumerate(test_texts):
        score_match = probs[i][1].item()
        
        # å®‰å…¨å¤„ç†ï¼Œé˜²æ­¢ä¹‹å‰æ²¡æ•è·çš„ NaN å¯¼è‡´ int() æŠ¥é”™
        if np.isnan(score_match):
            bar_len = 0
            score_str = "NaN"
        else:
            bar_len = int(score_match * 20)
            score_str = f"{score_match:.6f}"
            
        bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
        print(f"Text: '{text}'")
        print(f"Score: {score_str} | {bar}")
        print("-" * 30)

if __name__ == "__main__":
    run_inference()